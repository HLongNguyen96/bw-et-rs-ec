{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827827ba-ff90-4995-8721-a92bb0df36f4",
   "metadata": {},
   "source": [
    "<h1>Chapter 1</h1>\n",
    "<h2>Random forest reconstructing North / South monthly data</h2>\n",
    "\n",
    "<h2>1. Import package and Extract data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import netCDF4 as nc4\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# CONFIGURATION\n",
    "# File directory\n",
    "file_dir = \"Data/\" # Data to be downloaded from OzFlux data portal: https://data.ozflux.org.au/pub/viewColDetails.jspx?collection.id=1883250&collection.owner.id=768&viewType=anonymous\n",
    "\n",
    "# Years and NetCDF filenames\n",
    "years_all = range(2011, 2026)  # 2011â€“2025 inclusive\n",
    "netcdf_files = [f\"Gingin_{year}_L3.nc\" for year in years_all]\n",
    "\n",
    "# Time conversion (to Unix timestamp) - \"days since 1800-01-01\"\n",
    "convert_unix_800 = 5364691200\n",
    "\n",
    "# Random Forest (RF) settings\n",
    "rf_kwargs = dict(n_estimators=1000,random_state=42,n_jobs=-1,oob_score=True,)\n",
    "\n",
    "# Variables\n",
    "target_col = \"Et\"\n",
    "feature_cols = [\"Fsd\",\"Ws\",\"L\",\"sigmav\",\"ustar\",\"Fn\",\"Fg\",\"Sws_40cm\",\"Sws_80cm\",]\n",
    "wind_col = \"Wd\"\n",
    "\n",
    "# Year ranges\n",
    "yr_north = np.arange(2011,2026) \n",
    "yr_pre_fire = np.arange(2011,2017) \n",
    "yr_fire = np.arange(2016,2018) \n",
    "yr_post_fire = np.arange(2017,2026) \n",
    "months_all = np.arange(1,13)\n",
    "\n",
    "# EXTRACT FULL FLUX DATASET\n",
    "fc_list = []\n",
    "\n",
    "for fn in netcdf_files:\n",
    "    ds = nc4.Dataset(file_dir + fn)\n",
    "\n",
    "    # Time\n",
    "    time_arr = ds[\"time\"][:]\n",
    "    time_arr = time_arr.reshape(len(time_arr), 1).filled(np.nan)\n",
    "\n",
    "    time_df = pd.DataFrame(time_arr, columns=[\"datestamp_since_1800\"])\n",
    "    time_df[\"timestamp\"] = time_df[\"datestamp_since_1800\"] * 86400 - convert_unix_800\n",
    "\n",
    "    time_df[\"datetime\"] = time_df[\"timestamp\"].apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "    time_df[\"datetime\"] = pd.DatetimeIndex(time_df[\"datetime\"], tz=\"greenwich\")\n",
    "    time_df = time_df.drop(columns=[\"datestamp_since_1800\", \"timestamp\"])\n",
    "\n",
    "    # Variables\n",
    "    def _safe_var(name):\n",
    "        return ds[name][:].reshape(-1, 1).filled(np.nan)\n",
    "\n",
    "    data_cols = {\n",
    "        wind_col: _safe_var(wind_col),\n",
    "        \"Fsd\": _safe_var(\"Fsd\"),\n",
    "        \"Et\": _safe_var(\"Et\"),\n",
    "        \"Ws\": _safe_var(\"Ws\"),\n",
    "        \"L\": _safe_var(\"L\"),\n",
    "        \"sigmav\": _safe_var(\"sigmav\"),\n",
    "        \"ustar\": _safe_var(\"ustar\"),\n",
    "        \"Fn\": _safe_var(\"Fn\"),\n",
    "        \"Fg\": _safe_var(\"Fg\"),\n",
    "        \"Sws_40cm\": _safe_var(\"Sws_40cm\"),\n",
    "        \"Sws_80cm\": _safe_var(\"Sws_80cm\"),\n",
    "    }\n",
    "\n",
    "    data_df = pd.DataFrame(np.hstack(list(data_cols.values())),columns=list(data_cols.keys()))\n",
    "\n",
    "    fc_i = pd.concat([time_df.reset_index(drop=True),data_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    fc_i = fc_i.set_index(\"datetime\")\n",
    "    fc_i[\"year\"] = fc_i.index.year\n",
    "    fc_i[\"month\"] = fc_i.index.month\n",
    "    fc_i[\"day\"] = fc_i.index.day\n",
    "    fc_i[\"hour\"] = fc_i.index.hour\n",
    "\n",
    "    fc_list.append(fc_i)\n",
    "\n",
    "# Polish \n",
    "fc = pd.concat(fc_list).sort_index()\n",
    "\n",
    "# Remove bad data in September 2012\n",
    "fc_temp = fc[(fc.index.year == 2012) & (fc.index.month == 9)].copy()\n",
    "cols_nan = [\"Fsd\",\"Et\",\"Ws\",\"L\",\"sigmav\",\"ustar\",\"Fn\",\"Fg\",\"Sws_40cm\",\"Sws_80cm\",wind_col]\n",
    "fc_temp.loc[:, cols_nan] = np.nan\n",
    "\n",
    "fc = pd.concat([fc, fc_temp]).drop_duplicates(subset=[\"year\",\"month\",\"day\",\"hour\"], keep=\"last\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a724875",
   "metadata": {},
   "source": [
    "<h2>2. Train RF model using each north and south measurements and make predictons for the respective areas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda634b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMPTY MONTHLY OUTPUT TABELS\n",
    "def init_monthly_df(yrs):\n",
    "    idx = pd.MultiIndex.from_product(\n",
    "        [yrs.astype(float), months_all.astype(float)],\n",
    "        names=[\"year\",\"month\"]\n",
    "    )\n",
    "    return pd.DataFrame(np.nan, index=idx, columns=[\n",
    "        \"sum_prediction\",\n",
    "        \"sum_truth_train\",\n",
    "        \"sum_truth_cannot_predict\",\n",
    "        \"original_data_sum\",\n",
    "        \"truth_train_data_sum\",\n",
    "        \"truth_cannot_predict_data_sum\",\n",
    "        \"predicted_data_sum\",\n",
    "    ])\n",
    "\n",
    "fc_monthly_north = init_monthly_df(yr_north)\n",
    "fc_monthly_south_pre_fire = init_monthly_df(yr_pre_fire)\n",
    "fc_monthly_south_fire = init_monthly_df(yr_fire)\n",
    "fc_monthly_south_post_fire = init_monthly_df(yr_post_fire)\n",
    "\n",
    "# PARTITION LOGIC\n",
    "def _partition(df, site):\n",
    "    has_et = df[target_col].notna()\n",
    "    good_pred = df[feature_cols].notna().all(axis=1)\n",
    "\n",
    "    if site == \"north\":\n",
    "        # North: Wd <= 67.5 OR Wd >= 292.5\n",
    "        good_wind = (df[wind_col] <= 67.5) | (df[wind_col] >= 292.5)\n",
    "    elif site == \"south\":\n",
    "        # South: Wd between 112.5 and 247.5\n",
    "        good_wind = df[wind_col].between(112.5, 247.5)\n",
    "    else:\n",
    "        # Fallback: no wind filtering\n",
    "        good_wind = pd.Series(True, index=df.index)\n",
    "\n",
    "    truth_train_mask = has_et & good_pred & good_wind\n",
    "    truth_train = df[truth_train_mask]\n",
    "    truth_cannot = df[has_et & ~truth_train_mask]\n",
    "    to_predict = df[df[target_col].isna() & good_pred & good_wind]\n",
    "\n",
    "    return truth_train, truth_cannot, to_predict\n",
    "\n",
    "# COMBINED MONTH GROUPS\n",
    "north_groups = [\n",
    "    np.array([1,12]),\n",
    "    np.array([2,3,4,11]),\n",
    "    np.array([5,7,9]),\n",
    "    np.array([6,8,10]),\n",
    "]\n",
    "\n",
    "south_groups = [\n",
    "    np.array([1,2,3]),\n",
    "    np.array([4,6,7,8,10,11]),\n",
    "    np.array([5,9,12]),\n",
    "]\n",
    "\n",
    "# PROCESS PHASES (PRE-FIRE,FIRE,POST-FIRE)\n",
    "def process_phase(fc_phase, years, groups, df_out, site):\n",
    "    for group in groups:\n",
    "        fc_group = fc_phase[fc_phase.index.month.isin(group)]\n",
    "\n",
    "        # Global RF training for this group\n",
    "        truth_train_all, _, _ = _partition(fc_group, site=site)\n",
    "\n",
    "        if truth_train_all.empty:\n",
    "            rf = None\n",
    "            X_train = None\n",
    "        else:\n",
    "            X_train = truth_train_all[feature_cols].values\n",
    "            y_train = truth_train_all[target_col].values\n",
    "            rf = RandomForestRegressor(**rf_kwargs).fit(X_train, y_train)\n",
    "\n",
    "        for y in years:\n",
    "            fc_year = fc_group[fc_group.index.year == y]\n",
    "            if fc_year.empty:\n",
    "                continue\n",
    "\n",
    "            for m in group:\n",
    "                fc_month = fc_year[fc_year.index.month == m]\n",
    "                if fc_month.empty:\n",
    "                    continue\n",
    "\n",
    "                truth_train, truth_cannot, to_predict = _partition(fc_month, site=site)\n",
    "\n",
    "                idx = (float(y), float(m))\n",
    "\n",
    "                sum_truth_train = truth_train[target_col].sum()\n",
    "                sum_truth_cannot = truth_cannot[target_col].sum()\n",
    "                original_sum = sum_truth_train + sum_truth_cannot\n",
    "\n",
    "                if rf is None or X_train is None or to_predict.empty:\n",
    "                    sum_pred = np.nan\n",
    "                    pred_sum = np.nan\n",
    "                else:\n",
    "                    y_pred = rf.predict(to_predict[feature_cols].values)\n",
    "                    sum_pred = y_pred.sum()\n",
    "                    pred_sum = sum_pred\n",
    "\n",
    "                df_out.loc[idx] = [\n",
    "                    sum_pred,\n",
    "                    sum_truth_train,\n",
    "                    sum_truth_cannot,\n",
    "                    original_sum,\n",
    "                    sum_truth_train,\n",
    "                    sum_truth_cannot,\n",
    "                    pred_sum,\n",
    "                ]\n",
    "\n",
    "# NORTH\n",
    "process_phase(fc, yr_north, north_groups, fc_monthly_north, site=\"north\")\n",
    "\n",
    "# SOUTH PRE-FIRE\n",
    "fc_pre_fire = fc[\"2011-01-01\":\"2016-10-13\"]\n",
    "process_phase(fc_pre_fire, yr_pre_fire, south_groups, fc_monthly_south_pre_fire, site=\"south\")\n",
    "\n",
    "# SOUTH FIRE\n",
    "fc_fire = fc[\"2016-10-14\":\"2017-10-13\"]\n",
    "process_phase(fc_fire, yr_fire, south_groups, fc_monthly_south_fire, site=\"south\")\n",
    "\n",
    "# SOUTH POST-FIRE\n",
    "fc_post_fire = fc[\"2017-10-14\":]\n",
    "process_phase(fc_post_fire, yr_post_fire, south_groups, fc_monthly_south_post_fire, site=\"south\")\n",
    "\n",
    "# MERGE SOUTH PRE-FIRE, FIRE, AND POST-FIRE\n",
    "\n",
    "# Pre-fire and fire: Pre-fire row 69 and fire row 9\n",
    "pre_fire_plus_fire_sum = fc_monthly_south_pre_fire.loc[69,:] + fc_monthly_south_fire.loc[9,:]\n",
    "pre_fire_plus_fire_sum[['year','month']] = [2016,10]\n",
    "\n",
    "# Fire and post-fire: Fire row 21 and post-fire row 9\n",
    "fire_plus_post_fire_sum = fc_monthly_south_fire.loc[21,:] + fc_monthly_south_post_fire.loc[9,:]\n",
    "fire_plus_post_fire_sum[['year','month']] = [2017,10]\n",
    "\n",
    "# Merge\n",
    "fc_monthly_south = pd.concat([fc_monthly_south_pre_fire[68:],pre_fire_plus_fire_sum,fc_monthly_south_fire[10:20],fire_plus_post_fire_sum,fc_monthly_south_post_fire[10:]],axis=0).reset_index(drop=True)\n",
    "\n",
    "# POLISH\n",
    "fc_monthly_north['ET'] = ((fc_monthly_north['sum_prediction'] + fc_monthly_north['sum_truth_train'] + fc_monthly_north['sum_truth_cannot_predict'])/\n",
    "                          (fc_monthly_north['predicted_data_sum'] + fc_monthly_north['truth_train_data_sum'] + fc_monthly_north['truth_cannot_predict_data_sum']))\n",
    "fc_monthly_south['ET'] = ((fc_monthly_south['sum_prediction'] + fc_monthly_south['sum_truth_train'] + fc_monthly_south['sum_truth_cannot_predict'])/\n",
    "                          (fc_monthly_south['predicted_data_sum'] + fc_monthly_south['truth_train_data_sum'] + fc_monthly_south['truth_cannot_predict_data_sum']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
